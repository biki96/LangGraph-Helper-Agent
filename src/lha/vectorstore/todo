Ovo je dobar start (čisto, modularno, factory pattern), ali ima par stvari koje bih promenio da bude robustno + “LangChain v1 compliant” + praktično za offline/online mod.

Imaš 4 ključne tačke:

Šta je OK

odvojio si embeddings factory od vectorstore factory ✅

koristiš langchain_core interfejse (Embeddings, VectorStore, Document) ✅

query() kao generička operacija ✅

Šta bih popravio (važno)
1) langchain_chroma možda neće biti dostupno u LC v1 setup-u

U LC v1 svetu najčešće se koristi:

from langchain.vectorstores import Chroma

ili from langchain_community.vectorstores import Chroma

langchain_chroma je noviji split paket i može da te odvede u LC v2 kompatibilnost/zavrzlamu.

✅ Bezbednija varijanta za LC v1:

from langchain_community.vectorstores import Chroma


(I u dependencies obavezno langchain-community 0.0.x.)

2) index_to_chroma() treba da persistuje

Chroma.from_documents() napravi store, ali zavisi od verzije i konfiguracije da li odmah upiše na disk. Najsigurnije je:

vs = Chroma.from_documents(...)
vs.persist()
return vs

3) U query-u je bolje koristiti similarity_search_with_score

Za RAG i debugging (i evaluatore), dobro je da vidiš score.

Napravi:

query() → vraća docs

query_with_scores() → vraća (doc, score) listu

4) Stavi metadata i filtere u query

Dokumentacija ti dolazi iz 2 izvora (langgraph/langchain). Često želiš filter:

{"source": "langgraph_llms_full.txt"}

{"docset": "langgraph"}

Chroma podržava filter= (u LC wrappers često filter ili where zavisi). Najsigurnije: izloži filter: dict | None.

Predložena “polished” verzija (LC v1 friendly)
"""Modular vector store with pluggable embeddings and backends (LangChain v1 friendly)."""

from __future__ import annotations

from pathlib import Path
from typing import Any, Optional

from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.vectorstores import VectorStore

DEFAULT_COLLECTION = "langgraph_docs"
DEFAULT_K = 5


# --- Embedding Factories ---


def create_google_embeddings(
    api_key: str,
    model: str = "models/text-embedding-004",
) -> Embeddings:
    """Create Google Gemini embeddings."""
    from langchain_google_genai import GoogleGenerativeAIEmbeddings

    return GoogleGenerativeAIEmbeddings(model=model, google_api_key=api_key)


# --- VectorStore Factories ---


def create_chroma_store(
    embeddings: Embeddings,
    persist_dir: Path,
    collection_name: str = DEFAULT_COLLECTION,
) -> VectorStore:
    """Create or load a Chroma vector store from disk."""
    from langchain_community.vectorstores import Chroma

    persist_dir.mkdir(parents=True, exist_ok=True)
    return Chroma(
        collection_name=collection_name,
        embedding_function=embeddings,
        persist_directory=str(persist_dir),
    )


def index_to_chroma(
    documents: list[Document],
    embeddings: Embeddings,
    persist_dir: Path,
    collection_name: str = DEFAULT_COLLECTION,
) -> VectorStore:
    """Index documents into Chroma and persist to disk."""
    from langchain_community.vectorstores import Chroma

    persist_dir.mkdir(parents=True, exist_ok=True)

    vs = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=str(persist_dir),
    )
    # Ensure persistence (safe across versions)
    try:
        vs.persist()
    except Exception:
        pass
    return vs


# --- Generic Operations ---


def query(
    store: VectorStore,
    text: str,
    k: int = DEFAULT_K,
    filter: Optional[dict[str, Any]] = None,
) -> list[Document]:
    """Query relevant documents (optionally filtered by metadata)."""
    # Some backends accept filter=..., some ignore it.
    try:
        return store.similarity_search(text, k=k, filter=filter)
    except TypeError:
        return store.similarity_search(text, k=k)


def query_with_scores(
    store: VectorStore,
    text: str,
    k: int = DEFAULT_K,
    filter: Optional[dict[str, Any]] = None,
) -> list[tuple[Document, float]]:
    """Query relevant documents with scores (useful for debugging and eval)."""
    try:
        return store.similarity_search_with_score(text, k=k, filter=filter)
    except TypeError:
        return store.similarity_search_with_score(text, k=k)

Još jedna bitna stvar (za tvoj assignment)

Pošto imaš offline i online mode, u online modu možeš:

da koristiš web search i onda ubacuješ web rezultate kao “temp docs”

ali pazi: to ne treba da “zagadi” offline bazu

Rešenje:

offline Chroma kolekcije su read-only

online rezultate držiš u memory store ili posebnoj kolekciji web_cache koja se može obrisati

Zaključak

Da, tvoj kod je OK kao osnova, ali bih:

prešao na langchain_community.vectorstores.Chroma (LC v1)

dodao persist() posle indexing-a

dodao query_with_scores() i filter

Ako želiš, mogu ti predložiti i kako da napraviš 2 kolekcije (langgraph_docs, langchain_docs) i kako da radiš retrieval sa “routing”-om (npr. pitanje o StateGraph → prvo langgraph kolekcija).